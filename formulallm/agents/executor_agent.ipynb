{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import formulallm.formula as f\n",
    "from pathlib import Path\n",
    "from autogen.coding import CodeBlock, LocalCommandLineCodeExecutor\n",
    "from autogen import ConversableAgent, UserProxyAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = Path(\"coding\")\n",
    "work_dir.mkdir(exist_ok=True)\n",
    "\n",
    "executor = LocalCommandLineCodeExecutor(timeout = 60,\n",
    "                                        work_dir = work_dir,\n",
    "                                        functions = [f.load, f.solve, f.list, f.extract]\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm_config={\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"NotRequired\", # Loaded with LiteLLM command\n",
    "            \"api_key\": \"NotRequired\", # Not needed\n",
    "            \"base_url\": \"http://0.0.0.0:4000\"  # Your LiteLLM URL\n",
    "        }\n",
    "    ],\n",
    "    \"cache_seed\": None # Turns off caching, useful for testing different models\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Compiled) MappingExample.4ml\n",
      "0.59s.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/zhang0311/FormulaLLMPY/examples/data/MappingExample.4ml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor_agent = ConversableAgent(name=\"code_executor\",\n",
    "                                  system_message = '''You are a specialized code executor.\n",
    "                                  Your task is to first load the DSL model.\n",
    "                                  Then, solve and complete the partial models.\n",
    "                                  Next, extract the solution to the partial model and report whether the model is \"broken\".''',\n",
    "                                  llm_config = local_llm_config,\n",
    "                                  code_execution_config={\"executor\": executor},\n",
    "                                  human_input_mode=\"NEVER\",\n",
    "                                  is_termination_msg = lambda msg: \"Terminate\" in msg[\"content\"],\n",
    "                                  default_auto_reply= \"Please continue. If everything is done, reply 'TERMINATE'.\",\n",
    "                                  description = '''DSL Code Executor. \n",
    "                                                Solves and completes partial models in the loaded code, \n",
    "                                                extracts solutions, and reports whether the models are broken.'''\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "userProxy = UserProxyAgent(name=\"user_proxy\",\n",
    "                           system_message= '''Let the code_executor to run the \"solve\" command\n",
    "                                            to try to solve the partial models in the code loaded.\n",
    "                                            Ask the code_executor if the model is \"broken\"(unsolvable) or not.\n",
    "                                           ''',\n",
    "                           code_execution_config = False,\n",
    "                           llm_config = local_llm_config,\n",
    "                           human_input_mode=\"ALWAYS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_res = userProxy.initiate_chat(\n",
    "    recipient = executor_agent,\n",
    "    message = f\"Load the DSL model from the path({file_path}), then try to solve the model and report whether \n",
    "                the model is broken\",\n",
    "    summary_method = \"reflection_with_llm\",\n",
    "    summary_prompt = \"Summarize the conversation\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
